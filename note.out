\BOOKMARK [0][-]{chapter.1}{Mathematical Foundation}{}% 1
\BOOKMARK [1][-]{section.1.1}{Probability theory and mathematical statistics}{chapter.1}% 2
\BOOKMARK [2][-]{subsection.1.1.1}{How to get expected value and variance?}{section.1.1}% 3
\BOOKMARK [2][-]{subsection.1.1.2}{Discrete probability distribution}{section.1.1}% 4
\BOOKMARK [2][-]{subsection.1.1.3}{Continuous probability distribution}{section.1.1}% 5
\BOOKMARK [2][-]{subsection.1.1.4}{Sample mean and sample variance}{section.1.1}% 6
\BOOKMARK [0][-]{chapter.2}{Machine Learning}{}% 7
\BOOKMARK [1][-]{section.2.1}{Logistic regression \(LR\)}{chapter.2}% 8
\BOOKMARK [2][-]{subsection.2.1.1}{Form}{section.2.1}% 9
\BOOKMARK [1][-]{section.2.2}{Naive Bayes}{chapter.2}% 10
\BOOKMARK [2][-]{subsection.2.2.1}{Prior and posterior}{section.2.2}% 11
\BOOKMARK [2][-]{subsection.2.2.2}{Naive Bayesian Classifier \(NBC\)}{section.2.2}% 12
\BOOKMARK [2][-]{subsection.2.2.3}{Parameter estimation of NBC by maximum likelihood estimation \(MLE\)}{section.2.2}% 13
\BOOKMARK [1][-]{section.2.3}{Regularization}{chapter.2}% 14
\BOOKMARK [2][-]{subsection.2.3.1}{L0}{section.2.3}% 15
\BOOKMARK [2][-]{subsection.2.3.2}{L1 \(lasso regularization\)}{section.2.3}% 16
\BOOKMARK [2][-]{subsection.2.3.3}{L2 \(ridge regression or weight decay\)}{section.2.3}% 17
\BOOKMARK [1][-]{section.2.4}{Perceptron}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.4.1}{Why can't perceptron solve XOR problem?}{section.2.4}% 19
\BOOKMARK [2][-]{subsection.2.4.2}{Definition of perceptron}{section.2.4}% 20
\BOOKMARK [2][-]{subsection.2.4.3}{Learning Algorithm}{section.2.4}% 21
\BOOKMARK [2][-]{subsection.2.4.4}{Dual form of perceptron learning algorithm}{section.2.4}% 22
\BOOKMARK [1][-]{section.2.5}{Support vector machine \(SVM\)}{chapter.2}% 23
\BOOKMARK [2][-]{subsection.2.5.1}{Form}{section.2.5}% 24
\BOOKMARK [2][-]{subsection.2.5.2}{Lagrange duality}{section.2.5}% 25
\BOOKMARK [2][-]{subsection.2.5.3}{Solution of SVM}{section.2.5}% 26
\BOOKMARK [1][-]{section.2.6}{How to get the update rule of parameters of backpropagation in gradient descent algorithm?}{chapter.2}% 27
\BOOKMARK [1][-]{section.2.7}{Generative model and discriminative model}{chapter.2}% 28
\BOOKMARK [0][-]{chapter.3}{Deep Network}{}% 29
\BOOKMARK [1][-]{section.3.1}{How does backpropagation work?}{chapter.3}% 30
\BOOKMARK [1][-]{section.3.2}{Backpropagation of Convolutional Neural Network}{chapter.3}% 31
\BOOKMARK [2][-]{subsection.3.2.1}{Backpropagation of Fully Connectional Layer}{section.3.2}% 32
\BOOKMARK [2][-]{subsection.3.2.2}{Backpropagation of Convolutional Layer}{section.3.2}% 33
\BOOKMARK [1][-]{section.3.3}{Why does batch normalization work?}{chapter.3}% 34
\BOOKMARK [2][-]{subsection.3.3.1}{Dataset shift and covariate shift}{section.3.3}% 35
\BOOKMARK [2][-]{subsection.3.3.2}{Internal covariate shift}{section.3.3}% 36
\BOOKMARK [2][-]{subsection.3.3.3}{Batch normalization}{section.3.3}% 37
\BOOKMARK [1][-]{section.3.4}{Why does residual learning work?}{chapter.3}% 38
\BOOKMARK [0][-]{chapter.4}{Contents specifically referenced}{}% 39
