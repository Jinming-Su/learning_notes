\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Mathematical Foundation}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Advanced math}{1}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Taylor formula}{1}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Probability theory and mathematical statistics}{1}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}How to get expected value and variance?}{1}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Discrete probability distribution}{2}{subsection.1.2.2}}
\newlabel{eq:bernoulli_distribution_e_var}{{1.7}{2}{Discrete probability distribution}{equation.1.2.7}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Note in Eq.\nobreakspace  {}\ref  {eq:bernoulli_distribution_e_var}, maybe $P(X = 1)$ is equivalent $P(X^2 = 1^2)$, which ensures the establishment of this equation.}{2}{section*.3}}
\pgfsyspdfmark {pgfid1}{20112834}{26671280}
\newlabel{eq:binomial_distribution_e_var}{{1.9}{2}{Discrete probability distribution}{equation.1.2.9}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ There exists another solution directly through derivation of the probability mass function of Binomial distribution.}{2}{section*.4}}
\pgfsyspdfmark {pgfid2}{20112834}{9064831}
\newlabel{eq:poisson_distribution_e_var}{{1.11}{3}{Discrete probability distribution}{equation.1.2.11}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Note there exists Taylor Expansion (at $x = 0$) $e^x = 1 + \frac  {x}{1!} + \frac  {x^2}{2!} + \frac  {x^3}{3!} + \cdot \cdot \cdot + \cdot \cdot \cdot = \DOTSB \sum@ \slimits@ _{i=0}^{\infty } \frac  {x^i}{i!}$.}{3}{section*.5}}
\pgfsyspdfmark {pgfid3}{20112834}{36480224}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Continuous probability distribution}{3}{subsection.1.2.3}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The derivation of the expectation and invariance of normal distribution requires multiple integration operations.}{3}{section*.6}}
\pgfsyspdfmark {pgfid4}{20112834}{4276086}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The derivation of the expectation and invariance of exponential distribution requires multiple integration operations.}{4}{section*.7}}
\pgfsyspdfmark {pgfid5}{20112834}{38639065}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: conjugate distribution and Beta distribution.}{4}{section*.8}}
\pgfsyspdfmark {pgfid6}{20112834}{37480075}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Sample mean and sample variance}{4}{subsection.1.2.4}}
\newlabel{eq:unbiased_estimation_of_sample_mean_and_variance}{{1.19}{4}{Sample mean and sample variance}{equation.1.2.19}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Machine Learning}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Linear regression}{5}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Unitary linear regression}{5}{subsection.2.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Linear Gression Least mean square (LMS)}{5}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Logistic regression (LR)}{6}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Form}{6}{subsection.2.3.1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The is the opposite number of binary cross entropy loss function.}{6}{section*.9}}
\pgfsyspdfmark {pgfid7}{20112834}{47823067}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ It can be proved that the cross entropy function is a convex function. A detailed proof can be found in the blog \url  {https://blog.csdn.net/RHONYN/article/details/80342126}. About the proof of convex function, the first-order function relies on the non-negativity of the second derivative and the higher-order function relies on the semi-postive characterization of Hessian Matrix. Therefore, we can get the global minimum value of cross entropy.}{7}{section*.10}}
\pgfsyspdfmark {pgfid8}{20112834}{45480060}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Naive Bayes}{7}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Prior and posterior}{7}{subsection.2.4.1}}
\newlabel{eq:posterior}{{2.18}{7}{Prior and posterior}{equation.2.4.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Naive Bayesian Classifier (NBC)}{7}{subsection.2.4.2}}
\newlabel{eq:naive_bayesian_classifier}{{2.19}{7}{Naive Bayesian Classifier (NBC)}{equation.2.4.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Parameter estimation of NBC by maximum likelihood estimation (MLE)}{7}{subsection.2.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Regularization}{8}{section.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces $L_1$ norm and $L_2$ norm.}}{8}{figure.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}L0}{8}{subsection.2.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}L1 (lasso regularization)}{9}{subsection.2.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{Why does we usually use L1 to make the weights sparse instead of L0?}{9}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}L2 (ridge regression or weight decay)}{9}{subsection.2.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Perceptron}{9}{section.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Why can't perceptron solve XOR problem?}{9}{subsection.2.6.1}}
\newlabel{sect:perceptron}{{2.6.1}{9}{Why can't perceptron solve XOR problem?}{subsection.2.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces XOR problem.}}{9}{figure.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Definition of perceptron}{9}{subsection.2.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Learning Algorithm}{9}{subsection.2.6.3}}
\newlabel{eq:perceptron_loss}{{2.34}{10}{Learning Algorithm}{equation.2.6.34}{}}
\newlabel{eq:perceptron_loss}{{2.35}{10}{Learning Algorithm}{equation.2.6.35}{}}
\newlabel{eq:perceptron_loss}{{2.36}{10}{Learning Algorithm}{equation.2.6.36}{}}
\newlabel{eq:perceptron_parameter_update}{{2.38}{10}{Learning Algorithm}{equation.2.6.38}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The convergence of this algorithm about perceptron leraning can be proved, but it is not within the scope of this note at present.}{10}{section*.12}}
\pgfsyspdfmark {pgfid9}{20112834}{13255421}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Dual form of perceptron learning algorithm}{10}{subsection.2.6.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Support vector machine (SVM)}{11}{section.2.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Classification hyperplane.}}{11}{figure.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Form}{11}{subsection.2.7.1}}
\newlabel{eq:svm1}{{2.46}{11}{Form}{equation.2.7.46}{}}
\newlabel{eq:svm2}{{2.49}{12}{Form}{equation.2.7.49}{}}
\newlabel{eq:svm3}{{2.50}{12}{Form}{equation.2.7.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Lagrange duality}{12}{subsection.2.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Solution of SVM}{12}{subsection.2.7.3}}
\newlabel{eq:svm3}{{2.54}{12}{Solution of SVM}{equation.2.7.54}{}}
\newlabel{eq:svm_lagrangian}{{2.55}{12}{Solution of SVM}{equation.2.7.55}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ Why?}{13}{section*.13}}
\pgfsyspdfmark {pgfid10}{20112834}{47823067}
\newlabel{eq:svm_dual}{{2.56}{13}{Solution of SVM}{equation.2.7.56}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Sequential minimal optimization algorithm is used to efficiently solve the convex quadratic programming problem (as \ref  {eq:svm_dual}).}{13}{section*.14}}
\pgfsyspdfmark {pgfid11}{20112834}{22559223}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}Kernel}{13}{subsection.2.7.4}}
\@writefile{toc}{\contentsline {subsubsection}{Why can gaussian kernel map to infinite dimension in SVM?}{13}{section*.15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}How to get the update rule of parameters of backpropagation in gradient descent algorithm?}{14}{section.2.8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Gaussian mixture model (GMM)}{14}{section.2.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.1}Maximum likelihood estimation}{14}{subsection.2.9.1}}
\newlabel{eq:gmm_mle}{{2.65}{14}{Maximum likelihood estimation}{equation.2.9.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9.2}GMM and EM}{15}{subsection.2.9.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ The derivation of EM algorigthm}{15}{section*.16}}
\pgfsyspdfmark {pgfid12}{20112834}{20669143}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Principal components analysis (PCA)}{15}{section.2.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.1}Maximum variance theory}{15}{subsection.2.10.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10.2}PCA}{15}{subsection.2.10.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces EM algorithm.}}{16}{algorithm.1}}
\newlabel{alg:Framwork}{{1}{16}{GMM and EM}{algorithm.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Examples.}}{17}{figure.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.11}Hidden Markov model (HMM)}{17}{section.2.11}}
\newlabel{fig:hmm}{{2.11}{17}{Hidden Markov model (HMM)}{section.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces HMM}}{17}{figure.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.1}Definition}{17}{subsection.2.11.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.2}Three basic problems}{18}{subsection.2.11.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ forward-backward algorithm}{18}{section*.17}}
\pgfsyspdfmark {pgfid13}{20112834}{44923101}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Baum-Welch algorithm}{18}{section*.18}}
\pgfsyspdfmark {pgfid14}{20112834}{42027407}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Viterbi algorithm}{18}{section*.19}}
\pgfsyspdfmark {pgfid15}{20112834}{39153673}
\@writefile{toc}{\contentsline {section}{\numberline {2.12}Conditional random field (CRF)}{18}{section.2.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.1}Probabilistic undirected graphical model, or Markov random field}{18}{subsection.2.12.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.2}Conditional random field}{18}{subsection.2.12.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ CRF algorithm}{18}{section*.20}}
\pgfsyspdfmark {pgfid16}{20112834}{13462139}
\@writefile{toc}{\contentsline {section}{\numberline {2.13}Generative model and discriminative model}{18}{section.2.13}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: Random forest}{18}{section*.21}}
\pgfsyspdfmark {pgfid17}{20112834}{7765448}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Network}{19}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}How does backpropagation work?}{19}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural network.}}{19}{figure.3.1}}
\newlabel{eq:neural_network}{{3.1}{19}{How does backpropagation work?}{equation.3.1.1}{}}
\newlabel{eq:backpropagation_1}{{{BP1}}{20}{How does backpropagation work?}{AMS.22}{}}
\newlabel{eq:backpropagation_2}{{{BP2}}{20}{How does backpropagation work?}{AMS.23}{}}
\newlabel{eq:backpropagation_3}{{{BP3}}{20}{How does backpropagation work?}{AMS.24}{}}
\newlabel{eq:backpropagation_4}{{{BP4}}{20}{How does backpropagation work?}{AMS.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Backpropagation of Convolutional Neural Network}{21}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Backpropagation of Fully Connectional Layer}{21}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Backpropagation of Convolutional Layer}{21}{subsection.3.2.2}}
\newlabel{eq:convolutional_layer}{{3.9}{21}{Backpropagation of Convolutional Layer}{equation.3.2.9}{}}
\newlabel{eq:convolutional_layer_backpropagation_2}{{{CONV-BP2}}{21}{Backpropagation of Convolutional Layer}{AMS.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Why does batch normalization work?}{23}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Dataset shift and covariate shift}{23}{subsection.3.3.1}}
\newlabel{eq:covariate_shift}{{3.16}{23}{Dataset shift and covariate shift}{equation.3.3.16}{}}
\newlabel{eq:covariate_shift_2}{{3.18}{23}{Dataset shift and covariate shift}{equation.3.3.18}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{red}{}{red!25}{\leavevmode {\color  {red!25}o}}\ A more precise definition of \textbf  {covariate} can't be found.}{23}{section*.31}}
\pgfsyspdfmark {pgfid18}{20112834}{25847431}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Internal covariate shift}{23}{subsection.3.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Derivative of sigmoid function.}}{23}{figure.3.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ PCA whitening and ZCA Whitening.}{23}{section*.32}}
\pgfsyspdfmark {pgfid19}{20112834}{4276086}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Batch normalization}{24}{subsection.3.3.3}}
\newlabel{sect:batch-normalization}{{3.3.3}{24}{Batch normalization}{subsection.3.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Sigmoid function.}}{24}{figure.3.3}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Why is multi-layr linear network equivalent to the represetation ability of a single layer linear network.}{24}{section*.33}}
\pgfsyspdfmark {pgfid20}{20112834}{18651941}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Forward algorithm of batch normalization.}}{24}{algorithm.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ There exists $\DOTSB \sum@ \slimits@ _i$ for $y_i$ because gradient from each $y_i$ will backpropagate for $\gamma $ and the effect of summation is achieved.}{25}{section*.34}}
\pgfsyspdfmark {pgfid21}{20112834}{24868999}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Why does residual learning work?}{26}{section.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Residual Learning.}}{26}{figure.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Understanding Deconvolution \& bilinear interpolation}{26}{section.3.5}}
\newlabel{eq:matrix_form_of_convolution}{{3.27}{27}{Understanding Deconvolution \& bilinear interpolation}{equation.3.5.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Why is it called Transposed Convolution?}{27}{subsection.3.5.1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ checkerboard artifacts, gridding issues}{27}{section*.35}}
\pgfsyspdfmark {pgfid22}{20112834}{4276086}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Bilinear interpolation}}{28}{figure.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Bilinear Interpolation}{28}{subsection.3.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Activation Function}{29}{section.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Sigmoid}{29}{subsection.3.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}ReLU}{29}{subsection.3.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Parameter Initialization}{30}{section.3.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Why not initialized with all zeros?}{30}{subsection.3.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Why not initialized with Standard Gaussian?}{30}{subsection.3.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}xavier}{30}{subsection.3.7.3}}
\newlabel{eq:xavier_fp}{{3.35}{30}{xavier}{equation.3.7.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.4}msra}{31}{subsection.3.7.4}}
\newlabel{eq:msra_fp}{{3.38}{31}{msra}{equation.3.7.38}{}}
\newlabel{eq:activation_bp}{{3.41}{32}{msra}{equation.3.7.41}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ We can know that Xavier and Msra can both belong to uniform distribution or gaussian distribution.}{32}{section*.36}}
\pgfsyspdfmark {pgfid24}{20112834}{6405469}
\citation{hangli2012}
\citation{ioffe2015batch}
\bibstyle{plain}
\bibdata{RefNote}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Contents specifically referenced}{33}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{ioffe2015batch}{1}
\bibcite{hangli2012}{2}
