\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Mathematical Foundation}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Probability theory and mathematical statistics}{1}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}How to get expected value and variance?}{1}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Discrete probability distribution}{1}{subsection.1.1.2}}
\newlabel{eq:bernoulli_distribution_e_var}{{1.5}{2}{Discrete probability distribution}{equation.1.1.5}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Note in Eq.\nobreakspace  {}\ref  {eq:bernoulli_distribution_e_var}, maybe $P(X = 1)$ is equivalent $P(X^2 = 1^2)$, which ensures the establishment of this equation.}{2}{section*.3}}
\pgfsyspdfmark {pgfid1}{20112834}{43208979}
\newlabel{eq:binomial_distribution_e_var}{{1.7}{2}{Discrete probability distribution}{equation.1.1.7}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ There exists another solution directly through derivation of the probability mass function of Binomial distribution.}{2}{section*.4}}
\pgfsyspdfmark {pgfid2}{20112834}{26595551}
\newlabel{eq:poisson_distribution_e_var}{{1.9}{2}{Discrete probability distribution}{equation.1.1.9}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Note there exists Taylor Expansion (at $x = 0$) $e^x = 1 + \frac  {x}{1!} + \frac  {x^2}{2!} + \frac  {x^3}{3!} + \cdot \cdot \cdot + \cdot \cdot \cdot = \DOTSB \sum@ \slimits@ _{i=0}^{\infty } \frac  {x^i}{i!}$.}{2}{section*.5}}
\pgfsyspdfmark {pgfid3}{20112834}{9472589}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: Taylor's formula.}{2}{section*.6}}
\pgfsyspdfmark {pgfid4}{20112834}{8157917}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Continuous probability distribution}{2}{subsection.1.1.3}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The derivation of the expectation and invariance of normal distribution requires multiple integration operations.}{3}{section*.7}}
\pgfsyspdfmark {pgfid5}{20112834}{22858623}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The derivation of the expectation and invariance of exponential distribution requires multiple integration operations.}{3}{section*.8}}
\pgfsyspdfmark {pgfid6}{20112834}{13101020}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: Beta distribution.}{3}{section*.9}}
\pgfsyspdfmark {pgfid7}{20112834}{12009205}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Sample mean and sample variance}{3}{subsection.1.1.4}}
\newlabel{eq:unbiased_estimation_of_sample_mean_and_variance}{{1.17}{4}{Sample mean and sample variance}{equation.1.1.17}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Machine Learning}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Naive Bayes}{5}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Prior and posterior}{5}{subsection.2.1.1}}
\newlabel{eq:posterior}{{2.1}{5}{Prior and posterior}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Naive Bayesian Classifier (NBC)}{5}{subsection.2.1.2}}
\newlabel{eq:naive_bayesian_classifier}{{2.2}{5}{Naive Bayesian Classifier (NBC)}{equation.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Parameter estimation of NBC by maximum likelihood estimation (MLE)}{5}{subsection.2.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Perceptron}{6}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Why can't perceptron solve XOR problem?}{6}{subsection.2.2.1}}
\newlabel{sect:perceptron}{{2.2.1}{6}{Why can't perceptron solve XOR problem?}{subsection.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces XOR problem.}}{6}{figure.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Definition of perceptron}{6}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Learning Algorithm}{7}{subsection.2.2.3}}
\newlabel{eq:perceptron_loss}{{2.13}{7}{Learning Algorithm}{equation.2.2.13}{}}
\newlabel{eq:perceptron_loss}{{2.14}{7}{Learning Algorithm}{equation.2.2.14}{}}
\newlabel{eq:perceptron_loss}{{2.15}{7}{Learning Algorithm}{equation.2.2.15}{}}
\newlabel{eq:perceptron_parameter_update}{{2.17}{7}{Learning Algorithm}{equation.2.2.17}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The convergence of this algorithm about perceptron leraning can be proved, but it is not within the scope of this note at present.}{7}{section*.10}}
\pgfsyspdfmark {pgfid8}{20112834}{4669302}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Dual form of perceptron learning algorithm}{8}{subsection.2.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}How to get the update rule of parameters of backpropagation in gradient descent algorithm?}{8}{section.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Generative model and discriminative model}{8}{section.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Support vector machine}{8}{section.2.5}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: SVM}{8}{section*.11}}
\pgfsyspdfmark {pgfid9}{20112834}{5234370}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: Random forest}{8}{section*.12}}
\pgfsyspdfmark {pgfid10}{20112834}{4208911}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Network}{9}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}How does backpropagation work?}{9}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural network.}}{9}{figure.3.1}}
\newlabel{eq:neural_network}{{3.1}{9}{How does backpropagation work?}{equation.3.1.1}{}}
\newlabel{eq:backpropagation_1}{{{BP1}}{10}{How does backpropagation work?}{AMS.13}{}}
\newlabel{eq:backpropagation_2}{{{BP2}}{10}{How does backpropagation work?}{AMS.14}{}}
\newlabel{eq:backpropagation_3}{{{BP3}}{10}{How does backpropagation work?}{AMS.15}{}}
\newlabel{eq:backpropagation_4}{{{BP4}}{10}{How does backpropagation work?}{AMS.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Backpropagation of Convolutional Neural Network}{11}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Backpropagation of Fully Connectional Layer}{11}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Backpropagation of Convolutional Layer}{11}{subsection.3.2.2}}
\newlabel{eq:convolutional_layer}{{3.9}{11}{Backpropagation of Convolutional Layer}{equation.3.2.9}{}}
\newlabel{eq:convolutional_layer_backpropagation_2}{{{CONV-BP2}}{11}{Backpropagation of Convolutional Layer}{AMS.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Why does batch normalization work?}{13}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Dataset shift and covariate shift}{13}{subsection.3.3.1}}
\newlabel{eq:covariate_shift}{{3.16}{13}{Dataset shift and covariate shift}{equation.3.3.16}{}}
\newlabel{eq:covariate_shift_2}{{3.18}{13}{Dataset shift and covariate shift}{equation.3.3.18}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{red}{}{red!25}{\leavevmode {\color  {red!25}o}}\ A more precise definition of \textbf  {covariate} can't be found.}{13}{section*.22}}
\pgfsyspdfmark {pgfid11}{20112834}{25847431}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Internal covariate shift}{13}{subsection.3.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Derivative of sigmoid function.}}{13}{figure.3.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ PCA whitening and ZCA Whitening.}{13}{section*.23}}
\pgfsyspdfmark {pgfid12}{20112834}{4276086}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Batch normalization}{14}{subsection.3.3.3}}
\newlabel{sect:batch-normalization}{{3.3.3}{14}{Batch normalization}{subsection.3.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Sigmoid function.}}{14}{figure.3.3}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Why is multi-layr linear network equivalent to the represetation ability of a single layer linear network.}{14}{section*.24}}
\pgfsyspdfmark {pgfid13}{20112834}{18651941}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward algorithm of batch normalization.}}{14}{algorithm.1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ There exists $\DOTSB \sum@ \slimits@ _i$ for $y_i$ because gradient from each $y_i$ will backpropagate for $\gamma $ and the effect of summation is achieved.}{15}{section*.25}}
\pgfsyspdfmark {pgfid14}{20112834}{24868999}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Why does residual learning work?}{16}{section.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Residual Learning.}}{16}{figure.3.4}}
\citation{hangli2012}
\citation{ioffe2015batch}
\bibstyle{plain}
\bibdata{RefNote}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Contents specifically referenced}{17}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{ioffe2015batch}{1}
\bibcite{hangli2012}{2}
