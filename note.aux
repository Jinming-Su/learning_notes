\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Mathematical Foundation}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Probability theory and mathematical statistics}{1}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}How to get expected value and variance?}{1}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Discrete probability distribution}{1}{subsection.1.1.2}}
\newlabel{eq:bernoulli_distribution_e_var}{{1.5}{2}{Discrete probability distribution}{equation.1.1.5}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Note in Eq.\nobreakspace  {}\ref  {eq:bernoulli_distribution_e_var}, maybe $P(X = 1)$ is equivalent $P(X^2 = 1^2)$, which ensures the establishment of this equation.}{2}{section*.3}}
\pgfsyspdfmark {pgfid1}{20112834}{43058372}
\newlabel{eq:binomial_distribution_e_var}{{1.7}{2}{Discrete probability distribution}{equation.1.1.7}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ There exists another solution directly through derivation of the probability mass function of Binomial distribution.}{2}{section*.4}}
\pgfsyspdfmark {pgfid2}{20112834}{26930163}
\newlabel{eq:poisson_distribution_e_var}{{1.9}{2}{Discrete probability distribution}{equation.1.1.9}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Note there exists Taylor Expansion $e^x = 1 + \frac  {x}{1!} + \frac  {x^2}{2!} + \frac  {x^3}{3!} + \cdot \cdot \cdot + \cdot \cdot \cdot = \DOTSB \sum@ \slimits@ _{i=0}^{\infty } \frac  {x^i}{i!}$.}{2}{section*.5}}
\pgfsyspdfmark {pgfid3}{20112834}{9560187}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: Taylor's formula.}{2}{section*.6}}
\pgfsyspdfmark {pgfid4}{20112834}{8236202}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Continuous probability distribution}{2}{subsection.1.1.3}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The derivation of the expectation and invariance of normal distribution requires multiple integration operations.}{3}{section*.7}}
\pgfsyspdfmark {pgfid5}{20112834}{22858623}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The derivation of the expectation and invariance of exponential distribution requires multiple integration operations.}{3}{section*.8}}
\pgfsyspdfmark {pgfid6}{20112834}{13101020}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: Beta distribution.}{3}{section*.9}}
\pgfsyspdfmark {pgfid7}{20112834}{12009205}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Sample mean and sample variance}{3}{subsection.1.1.4}}
\newlabel{eq:unbiased_estimation_of_sample_mean_and_variance}{{1.17}{4}{Sample mean and sample variance}{equation.1.1.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Prior and posterior}{4}{section.1.2}}
\newlabel{eq:posterior}{{1.18}{4}{Prior and posterior}{equation.1.2.18}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Machine Learning}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Why can't perceptron solve XOR problem?}{5}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces XOR problem.}}{5}{figure.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Definition of perceptron}{5}{subsection.2.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Learning Algorithm}{5}{subsection.2.1.2}}
\newlabel{eq:perceptron_loss}{{2.4}{5}{Learning Algorithm}{equation.2.1.4}{}}
\newlabel{eq:perceptron_loss}{{2.5}{6}{Learning Algorithm}{equation.2.1.5}{}}
\newlabel{eq:perceptron_loss}{{2.6}{6}{Learning Algorithm}{equation.2.1.6}{}}
\newlabel{eq:perceptron_parameter_update}{{2.8}{6}{Learning Algorithm}{equation.2.1.8}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The convergence of this algorithm about perceptron leraning can be proved, but it is not within the scope of this note at present.}{6}{section*.10}}
\pgfsyspdfmark {pgfid8}{20112834}{17289384}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Dual form of perceptron learning algorithm}{6}{subsection.2.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}How to get the update rule of parameters of backpropagation in gradient descent algorithm?}{7}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Generative model and discriminative model}{7}{section.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Support vector machine}{7}{section.2.4}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: SVM}{7}{section*.11}}
\pgfsyspdfmark {pgfid9}{20112834}{14715134}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: Random forest}{7}{section*.12}}
\pgfsyspdfmark {pgfid10}{20112834}{13689675}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Network}{9}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}How does backpropagation work?}{9}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural network.}}{9}{figure.3.1}}
\newlabel{eq:neural_network}{{3.1}{9}{How does backpropagation work?}{equation.3.1.1}{}}
\newlabel{eq:backpropagation_1}{{{BP1}}{10}{How does backpropagation work?}{AMS.13}{}}
\newlabel{eq:backpropagation_2}{{{BP2}}{10}{How does backpropagation work?}{AMS.14}{}}
\newlabel{eq:backpropagation_3}{{{BP3}}{10}{How does backpropagation work?}{AMS.15}{}}
\newlabel{eq:backpropagation_4}{{{BP4}}{10}{How does backpropagation work?}{AMS.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Why does batch normalization work?}{11}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Dataset shift and covariate shift}{11}{subsection.3.2.1}}
\newlabel{eq:covariate_shift}{{3.8}{11}{Dataset shift and covariate shift}{equation.3.2.8}{}}
\newlabel{eq:covariate_shift_2}{{3.10}{11}{Dataset shift and covariate shift}{equation.3.2.10}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{red}{}{red!25}{\leavevmode {\color  {red!25}o}}\ A more precise definition of \textbf  {covariate} can't be found.}{11}{section*.17}}
\pgfsyspdfmark {pgfid11}{20112834}{27680460}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Internal covariate shift}{11}{subsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Derivative of sigmoid function.}}{11}{figure.3.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ PCA whitening and ZCA Whitening.}{11}{section*.18}}
\pgfsyspdfmark {pgfid12}{20112834}{4276086}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Batch normalization}{12}{subsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Sigmoid function.}}{12}{figure.3.3}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Why is multi-layr linear network equivalent to the represetation ability of a single layer linear network.}{12}{section*.19}}
\pgfsyspdfmark {pgfid13}{20112834}{18651941}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward algorithm of batch normalization.}}{12}{algorithm.1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ There exists $\DOTSB \sum@ \slimits@ _i$ for $y_i$ because gradient from each $y_i$ will backpropagate for $\gamma $ and the effect of summation is achieved.}{13}{section*.20}}
\pgfsyspdfmark {pgfid14}{20112834}{24868999}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Why does residual learning work?}{14}{section.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Residual Learning.}}{14}{figure.3.4}}
\citation{hangli2012}
\citation{ioffe2015batch}
\bibstyle{plain}
\bibdata{RefNote}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Contents specifically referenced}{15}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{ioffe2015batch}{1}
\bibcite{hangli2012}{2}
