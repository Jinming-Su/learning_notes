\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Mathematical Foundation}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Probability theory and mathematical statistics}{1}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}How to get expected value and variance?}{1}{subsection.1.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Discrete probability distribution}{1}{subsection.1.1.2}}
\newlabel{eq:bernoulli_distribution_e_var}{{1.5}{2}{Discrete probability distribution}{equation.1.1.5}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Note in Eq.\nobreakspace  {}\ref  {eq:bernoulli_distribution_e_var}, maybe $P(X = 1)$ is equivalent $P(X^2 = 1^2)$, which ensures the establishment of this equation.}{2}{section*.3}}
\pgfsyspdfmark {pgfid1}{20112834}{43208979}
\newlabel{eq:binomial_distribution_e_var}{{1.7}{2}{Discrete probability distribution}{equation.1.1.7}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ There exists another solution directly through derivation of the probability mass function of Binomial distribution.}{2}{section*.4}}
\pgfsyspdfmark {pgfid2}{20112834}{26595551}
\newlabel{eq:poisson_distribution_e_var}{{1.9}{2}{Discrete probability distribution}{equation.1.1.9}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Note there exists Taylor Expansion (at $x = 0$) $e^x = 1 + \frac  {x}{1!} + \frac  {x^2}{2!} + \frac  {x^3}{3!} + \cdot \cdot \cdot + \cdot \cdot \cdot = \DOTSB \sum@ \slimits@ _{i=0}^{\infty } \frac  {x^i}{i!}$.}{2}{section*.5}}
\pgfsyspdfmark {pgfid3}{20112834}{9472589}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: Taylor's formula.}{2}{section*.6}}
\pgfsyspdfmark {pgfid4}{20112834}{8157917}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Continuous probability distribution}{2}{subsection.1.1.3}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The derivation of the expectation and invariance of normal distribution requires multiple integration operations.}{3}{section*.7}}
\pgfsyspdfmark {pgfid5}{20112834}{21430744}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The derivation of the expectation and invariance of exponential distribution requires multiple integration operations.}{3}{section*.8}}
\pgfsyspdfmark {pgfid6}{20112834}{10699724}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: conjugate distribution and Beta distribution.}{3}{section*.9}}
\pgfsyspdfmark {pgfid7}{20112834}{9508484}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Sample mean and sample variance}{3}{subsection.1.1.4}}
\newlabel{eq:unbiased_estimation_of_sample_mean_and_variance}{{1.17}{4}{Sample mean and sample variance}{equation.1.1.17}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Machine Learning}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Logistic regression (LR)}{5}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Form}{5}{subsection.2.1.1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The is the opposite number of binary cross entropy loss function.}{5}{section*.10}}
\pgfsyspdfmark {pgfid8}{20112834}{7690223}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Naive Bayes}{6}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Prior and posterior}{6}{subsection.2.2.1}}
\newlabel{eq:posterior}{{2.10}{6}{Prior and posterior}{equation.2.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Naive Bayesian Classifier (NBC)}{6}{subsection.2.2.2}}
\newlabel{eq:naive_bayesian_classifier}{{2.11}{6}{Naive Bayesian Classifier (NBC)}{equation.2.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Parameter estimation of NBC by maximum likelihood estimation (MLE)}{6}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Regularization}{7}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces $L_1$ norm and $L_2$ norm.}}{7}{figure.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}L0}{7}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}L1 (lasso regularization)}{7}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Why does we usually use L1 to make the weights sparse instead of L0?}{7}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}L2 (ridge regression or weight decay)}{8}{subsection.2.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Perceptron}{8}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Why can't perceptron solve XOR problem?}{8}{subsection.2.4.1}}
\newlabel{sect:perceptron}{{2.4.1}{8}{Why can't perceptron solve XOR problem?}{subsection.2.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces XOR problem.}}{8}{figure.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Definition of perceptron}{8}{subsection.2.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Learning Algorithm}{8}{subsection.2.4.3}}
\newlabel{eq:perceptron_loss}{{2.26}{8}{Learning Algorithm}{equation.2.4.26}{}}
\newlabel{eq:perceptron_loss}{{2.27}{9}{Learning Algorithm}{equation.2.4.27}{}}
\newlabel{eq:perceptron_loss}{{2.28}{9}{Learning Algorithm}{equation.2.4.28}{}}
\newlabel{eq:perceptron_parameter_update}{{2.30}{9}{Learning Algorithm}{equation.2.4.30}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ The convergence of this algorithm about perceptron leraning can be proved, but it is not within the scope of this note at present.}{9}{section*.12}}
\pgfsyspdfmark {pgfid9}{20112834}{18075816}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Dual form of perceptron learning algorithm}{9}{subsection.2.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Support vector machine (SVM)}{10}{section.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Classification hyperplane.}}{10}{figure.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Form}{10}{subsection.2.5.1}}
\newlabel{eq:svm1}{{2.38}{10}{Form}{equation.2.5.38}{}}
\newlabel{eq:svm2}{{2.41}{11}{Form}{equation.2.5.41}{}}
\newlabel{eq:svm3}{{2.42}{11}{Form}{equation.2.5.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Lagrange duality}{11}{subsection.2.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Solution of SVM}{11}{subsection.2.5.3}}
\newlabel{eq:svm3}{{2.46}{11}{Solution of SVM}{equation.2.5.46}{}}
\newlabel{eq:svm_lagrangian}{{2.47}{11}{Solution of SVM}{equation.2.5.47}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ Why?}{11}{section*.13}}
\pgfsyspdfmark {pgfid10}{20112834}{4276086}
\newlabel{eq:svm_dual}{{2.48}{12}{Solution of SVM}{equation.2.5.48}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ Sequential minimal optimization algorithm is used to efficiently solve the convex quadratic programming problem (as \ref  {eq:svm_dual}).}{12}{section*.14}}
\pgfsyspdfmark {pgfid11}{20112834}{29409467}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}How to get the update rule of parameters of backpropagation in gradient descent algorithm?}{12}{section.2.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Generative model and discriminative model}{12}{section.2.7}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Todo: Random forest}{12}{section*.15}}
\pgfsyspdfmark {pgfid12}{20112834}{4208911}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Network}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}How does backpropagation work?}{13}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural network.}}{13}{figure.3.1}}
\newlabel{eq:neural_network}{{3.1}{13}{How does backpropagation work?}{equation.3.1.1}{}}
\newlabel{eq:backpropagation_1}{{{BP1}}{14}{How does backpropagation work?}{AMS.16}{}}
\newlabel{eq:backpropagation_2}{{{BP2}}{14}{How does backpropagation work?}{AMS.17}{}}
\newlabel{eq:backpropagation_3}{{{BP3}}{14}{How does backpropagation work?}{AMS.18}{}}
\newlabel{eq:backpropagation_4}{{{BP4}}{14}{How does backpropagation work?}{AMS.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Backpropagation of Convolutional Neural Network}{15}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Backpropagation of Fully Connectional Layer}{15}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Backpropagation of Convolutional Layer}{15}{subsection.3.2.2}}
\newlabel{eq:convolutional_layer}{{3.9}{15}{Backpropagation of Convolutional Layer}{equation.3.2.9}{}}
\newlabel{eq:convolutional_layer_backpropagation_2}{{{CONV-BP2}}{15}{Backpropagation of Convolutional Layer}{AMS.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Why does batch normalization work?}{17}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Dataset shift and covariate shift}{17}{subsection.3.3.1}}
\newlabel{eq:covariate_shift}{{3.16}{17}{Dataset shift and covariate shift}{equation.3.3.16}{}}
\newlabel{eq:covariate_shift_2}{{3.18}{17}{Dataset shift and covariate shift}{equation.3.3.18}{}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{red}{}{red!25}{\leavevmode {\color  {red!25}o}}\ A more precise definition of \textbf  {covariate} can't be found.}{17}{section*.25}}
\pgfsyspdfmark {pgfid13}{20112834}{25847431}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Internal covariate shift}{17}{subsection.3.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Derivative of sigmoid function.}}{17}{figure.3.2}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ PCA whitening and ZCA Whitening.}{17}{section*.26}}
\pgfsyspdfmark {pgfid14}{20112834}{4276086}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Batch normalization}{18}{subsection.3.3.3}}
\newlabel{sect:batch-normalization}{{3.3.3}{18}{Batch normalization}{subsection.3.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Sigmoid function.}}{18}{figure.3.3}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{Plum}{}{Plum!25}{\leavevmode {\color  {Plum!25}o}}\ Why is multi-layr linear network equivalent to the represetation ability of a single layer linear network.}{18}{section*.27}}
\pgfsyspdfmark {pgfid15}{20112834}{18651941}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward algorithm of batch normalization.}}{18}{algorithm.1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{yellow}{}{yellow!25}{\leavevmode {\color  {yellow!25}o}}\ There exists $\DOTSB \sum@ \slimits@ _i$ for $y_i$ because gradient from each $y_i$ will backpropagate for $\gamma $ and the effect of summation is achieved.}{19}{section*.28}}
\pgfsyspdfmark {pgfid16}{20112834}{24868999}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Why does residual learning work?}{20}{section.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Residual Learning.}}{20}{figure.3.4}}
\citation{hangli2012}
\citation{ioffe2015batch}
\bibstyle{plain}
\bibdata{RefNote}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Contents specifically referenced}{21}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{ioffe2015batch}{1}
\bibcite{hangli2012}{2}
