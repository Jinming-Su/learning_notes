\contentsline {chapter}{\numberline {1}Mathematical Foundation}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Advanced math}{1}{section.1.1}
\contentsline {subsection}{\numberline {1.1.1}Taylor formula}{1}{subsection.1.1.1}
\contentsline {section}{\numberline {1.2}Probability theory and mathematical statistics}{1}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}How to get expected value and variance?}{1}{subsection.1.2.1}
\contentsline {subsection}{\numberline {1.2.2}Discrete probability distribution}{2}{subsection.1.2.2}
\contentsline {subsection}{\numberline {1.2.3}Continuous probability distribution}{3}{subsection.1.2.3}
\contentsline {subsection}{\numberline {1.2.4}Sample mean and sample variance}{4}{subsection.1.2.4}
\contentsline {chapter}{\numberline {2}Machine Learning}{5}{chapter.2}
\contentsline {section}{\numberline {2.1}Logistic regression (LR)}{5}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Form}{5}{subsection.2.1.1}
\contentsline {section}{\numberline {2.2}Naive Bayes}{6}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Prior and posterior}{6}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Naive Bayesian Classifier (NBC)}{6}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Parameter estimation of NBC by maximum likelihood estimation (MLE)}{6}{subsection.2.2.3}
\contentsline {section}{\numberline {2.3}Regularization}{7}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}L0}{7}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}L1 (lasso regularization)}{7}{subsection.2.3.2}
\contentsline {subsubsection}{Why does we usually use L1 to make the weights sparse instead of L0?}{7}{section*.10}
\contentsline {subsection}{\numberline {2.3.3}L2 (ridge regression or weight decay)}{8}{subsection.2.3.3}
\contentsline {section}{\numberline {2.4}Perceptron}{8}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Why can't perceptron solve XOR problem?}{8}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}Definition of perceptron}{8}{subsection.2.4.2}
\contentsline {subsection}{\numberline {2.4.3}Learning Algorithm}{8}{subsection.2.4.3}
\contentsline {subsection}{\numberline {2.4.4}Dual form of perceptron learning algorithm}{9}{subsection.2.4.4}
\contentsline {section}{\numberline {2.5}Support vector machine (SVM)}{10}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Form}{10}{subsection.2.5.1}
\contentsline {subsection}{\numberline {2.5.2}Lagrange duality}{11}{subsection.2.5.2}
\contentsline {subsection}{\numberline {2.5.3}Solution of SVM}{11}{subsection.2.5.3}
\contentsline {subsection}{\numberline {2.5.4}Kernel}{12}{subsection.2.5.4}
\contentsline {subsubsection}{Why can gaussian kernel map to infinite dimension in SVM?}{12}{section*.14}
\contentsline {section}{\numberline {2.6}How to get the update rule of parameters of backpropagation in gradient descent algorithm?}{12}{section.2.6}
\contentsline {section}{\numberline {2.7}Gaussian mixture model (GMM)}{13}{section.2.7}
\contentsline {subsection}{\numberline {2.7.1}Maximum likelihood estimation}{13}{subsection.2.7.1}
\contentsline {subsection}{\numberline {2.7.2}GMM and EM}{14}{subsection.2.7.2}
\contentsline {section}{\numberline {2.8}Principal components analysis (PCA)}{14}{section.2.8}
\contentsline {subsection}{\numberline {2.8.1}Maximum variance theory}{14}{subsection.2.8.1}
\contentsline {subsection}{\numberline {2.8.2}PCA}{15}{subsection.2.8.2}
\contentsline {section}{\numberline {2.9}Generative model and discriminative model}{16}{section.2.9}
\contentsline {chapter}{\numberline {3}Deep Network}{17}{chapter.3}
\contentsline {section}{\numberline {3.1}How does backpropagation work?}{17}{section.3.1}
\contentsline {section}{\numberline {3.2}Backpropagation of Convolutional Neural Network}{19}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Backpropagation of Fully Connectional Layer}{19}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Backpropagation of Convolutional Layer}{19}{subsection.3.2.2}
\contentsline {section}{\numberline {3.3}Why does batch normalization work?}{21}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Dataset shift and covariate shift}{21}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Internal covariate shift}{21}{subsection.3.3.2}
\contentsline {subsection}{\numberline {3.3.3}Batch normalization}{22}{subsection.3.3.3}
\contentsline {section}{\numberline {3.4}Why does residual learning work?}{24}{section.3.4}
\contentsline {chapter}{\numberline {4}Contents specifically referenced}{25}{chapter.4}
